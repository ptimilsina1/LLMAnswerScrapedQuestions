{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc76a1f-c7db-4463-b03a-c52e069676bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prata\\Documents\\YOJAI\\Projects\\RAG\\.RAG_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from fpdf import FPDF\n",
    "import creds as cr\n",
    "# Initialize the OpenAI model with your API key\n",
    "openai_api_key = cr.openai_api_key  # Replace with your actual OpenAI API key\n",
    "llm = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455e2a43-52f2-4873-975b-277c2a1e3a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://devinterview.io/questions/machine-learning-and-data-science/anomaly-detection-interview-questions/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prata\\Documents\\YOJAI\\Projects\\RAG\\.RAG_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def __init__(self, topic_text, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.topic_text = topic_text\n",
    "        self.is_first_page = True\n",
    "\n",
    "    def header(self):\n",
    "        if self.is_first_page:\n",
    "            # Center the topic text on the first page\n",
    "            self.set_font(\"Arial\", 'B', size=20)\n",
    "            self.cell(0, 5, self.topic_text, ln=True, align='C')\n",
    "            self.ln(10)  # Add space after the topic text on the first page\n",
    "            self.is_first_page = False\n",
    "        else:\n",
    "            self.ln(10)  # Add space at the top of subsequent pages\n",
    "\n",
    "    def footer(self):\n",
    "        self.set_y(-15)  # Position from bottom\n",
    "        self.set_font(\"Arial\", 'I', size=8)\n",
    "        self.cell(0, 10, f'Page {self.page_no()} / {{nb}}', 0, 0, 'C')\n",
    "\n",
    "def replace_non_latin_chars(text):\n",
    "    return text.encode('latin-1', 'replace').decode('latin-1')\n",
    "\n",
    "def get_questions_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    questions = soup.find_all('li')\n",
    "    extracted_questions = []\n",
    "    for question in questions:\n",
    "        question_str = str(question)\n",
    "        question_str = re.sub(r'</?em>', ' ', question_str)\n",
    "        question_soup = BeautifulSoup(question_str, 'html.parser')\n",
    "        text = question_soup.get_text(strip=True)\n",
    "        cleaned_text = text.replace(\"QuestionAnswer:\", \"\").strip()\n",
    "        extracted_questions.append(cleaned_text)\n",
    "    return extracted_questions\n",
    "\n",
    "\n",
    "def get_complete_answer(question, topic):\n",
    "    max_context_length = 4097 - 256  # Model's max context length minus completion length\n",
    "    prompt_template = PromptTemplate(input_variables=[\"question\", \"topic\"], template=\"Topic: {topic}\\nQ: {question}\\nA:\")\n",
    "    topic_text = topic.split('-')[:-2]    \n",
    "    topic_text = \" \".join(str(item) for item in topic_text) \n",
    "    prompt = prompt_template.format(question=question, topic=topic_text)\n",
    "    \n",
    "    if len(prompt.split()) > max_context_length:\n",
    "        prompt = ' '.join(prompt.split()[:max_context_length])\n",
    "    \n",
    "    complete_answer = \"\"\n",
    "    while True:\n",
    "        response = llm(prompt + complete_answer)\n",
    "        new_text = response.strip()\n",
    "        if new_text.endswith('.') or len(new_text.split()) < 5:\n",
    "            complete_answer += new_text\n",
    "            break\n",
    "        else:\n",
    "            complete_answer += new_text + ' '\n",
    "    return complete_answer\n",
    "\n",
    "def write_to_pdf(filename, topic_text, questions_and_answers):\n",
    "    pdf = PDF(topic_text)\n",
    "    pdf.alias_nb_pages()  # Add this to use {nb} in footer for total page count\n",
    "    pdf.add_page()\n",
    "    \n",
    "    for question, answer in questions_and_answers:\n",
    "        pdf.set_font(\"Arial\", 'B', size=12)\n",
    "        pdf.multi_cell(0, 5, replace_non_latin_chars(f\"{question}\"))\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "        pdf.multi_cell(0, 5, replace_non_latin_chars(f\"{answer}\"))\n",
    "        pdf.ln(5)  # Add a new line for separation\n",
    "\n",
    "    pdf.output(filename, 'F')\n",
    "\n",
    "# Example usage\n",
    "base_url = \"https://devinterview.io\"\n",
    "url = \"https://devinterview.io/questions/machine-learning-and-data-science/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Locate the <a> tags with the class 'TopicButton'\n",
    "topics = soup.find_all('a', class_='TopicButton')\n",
    "\n",
    "# Ensure the data folder exists\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Iterate through each topic link\n",
    "for topic in topics:\n",
    "    topic_text = topic.get_text(strip=True)\n",
    "    topic_href = topic.get('href')\n",
    "    \n",
    "    new_url = base_url + topic_href\n",
    "    print(f\"Processing {new_url}\")\n",
    "    \n",
    "    questions = get_questions_from_url(new_url)\n",
    "\n",
    "    # Create a filename based on the topic URL\n",
    "    topic_name = topic_href.split('/')[-2]\n",
    "    \n",
    "    filename = os.path.join('data', f\"{topic_name}_with_answers.pdf\")\n",
    "\n",
    "    questions_and_answers = []\n",
    "    for question in questions:\n",
    "        answer = get_complete_answer(question, topic_name)\n",
    "        questions_and_answers.append((question, answer))\n",
    "\n",
    "    write_to_pdf(filename, topic_text, questions_and_answers)\n",
    "\n",
    "    print(f\"Questions and answers have been written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c28070e6-6bf9-43e8-ad9c-d5af6a46a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual topics data questions and answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0220e-db0a-4224-8a31-12e0f265d5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
